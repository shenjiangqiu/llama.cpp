/*Warning, this file is autogenerated by cbindgen. Don't modify this manually. */

#include <stdarg.h>
#include <stdbool.h>
#include <stdint.h>
#include <stdlib.h>

/**
 * the Block type are created in lamma.cpp, use save_tensor in llama.cpp
 */
typedef struct BlockQ2K {
  uint8_t scales[16];
  uint8_t qs[64];
  uint16_t d;
  uint16_t dmin;
} BlockQ2K;

/**
 * the Block type are created in lamma.cpp, use save_tensor in llama.cpp
 */
typedef struct BlockQ3K {
  uint8_t hmask[32];
  uint8_t qs[64];
  uint8_t scales[12];
  uint16_t d;
} BlockQ3K;

/**
 * the Block type are created in lamma.cpp, use save_tensor in llama.cpp
 */
typedef struct BlockQ4K {
  uint16_t d;
  uint16_t dmin;
  uint8_t scales[12];
  uint8_t qs[128];
} BlockQ4K;

/**
 * the Block type are created in lamma.cpp, use save_tensor in llama.cpp
 */
typedef struct BlockQ5K {
  uint16_t d;
  uint16_t dmin;
  uint8_t scales[12];
  uint8_t qh[32];
  uint8_t qs[128];
} BlockQ5K;

/**
 * the Block type are created in lamma.cpp, use save_tensor in llama.cpp
 */
typedef struct BlockQ6K {
  uint8_t ql[128];
  uint8_t qh[64];
  uint8_t scales[16];
  uint16_t d;
} BlockQ6K;

/**
 * the Block type are created in lamma.cpp, use save_tensor in llama.cpp
 */
typedef struct BlockQ8K {
  float d;
  int8_t qs[256];
  int16_t bsum[16];
} BlockQ8K;

typedef struct AttentionContext {

} AttentionContext;

#ifdef __cplusplus
extern "C" {
#endif // __cplusplus

/**
 * save the data into the global map
 */
void save_tensor_continouse_q2(const struct BlockQ2K *data, uintptr_t size, const char *name);

/**
 * save the data into the global map
 */
void save_tensor_continouse_q3(const struct BlockQ3K *data, uintptr_t size, const char *name);

/**
 * save the data into the global map
 */
void save_tensor_continouse_q4(const struct BlockQ4K *data, uintptr_t size, const char *name);

/**
 * save the data into the global map
 */
void save_tensor_continouse_q5(const struct BlockQ5K *data, uintptr_t size, const char *name);

/**
 * save the data into the global map
 */
void save_tensor_continouse_q6(const struct BlockQ6K *data, uintptr_t size, const char *name);

/**
 * save the data into the global map
 */
void save_tensor_continouse_q8(const struct BlockQ8K *data, uintptr_t size, const char *name);

void save_file(void);

/**
 * create a new name by concating two names, remember to call `free_name` to free the memory
 */
char *creat_concated_name(const char *name_1, const char *name_2);

/**
 * free the memory created by `creat_concated_name`
 */
void free_name(char *name);

void register_mul_mat(uint8_t src_0_bits,
                      uint8_t src_1_bits,
                      uintptr_t src_0_ne0,
                      uintptr_t src_0_ne1,
                      uintptr_t src_1_ne0,
                      uintptr_t src_1_ne1,
                      const char *src_0_name,
                      const char *src_1_name);

void init_logger_asni(void);

void init_logger(void);

void set_enable_save(bool enable);

bool get_enable_save(void);

void enable_save(void);

void disable_save(void);

void print_vector_q2_raw(const char *data, uintptr_t size);

void print_vector_q3_raw(const char *data, uintptr_t size);

void print_vector_q8_raw(const char *data, uintptr_t size);

void log_info(const char *data);

void log_debug(const char *data);

void print_vector_q2(const uint8_t (*array)[64]);

void print_vector_q3(const uint8_t (*array)[64], const uint8_t (*hmask)[32]);

void print_vector_q8(const int8_t (*array)[64]);

void print_vec(const int8_t (*array)[256]);

void print_vec_u8(const uint8_t (*array)[256]);

/**
 * update the data, policys:
 * - 0: shift
 * - 1: minus
 * - 2: shift and minus
 */
void update_data(uint8_t (*array)[256], uint8_t policy);

void save_attention_result(struct AttentionContext *context);

#ifdef __cplusplus
} // extern "C"
#endif // __cplusplus
